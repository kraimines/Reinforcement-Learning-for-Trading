# ğŸ¤– Reinforcement Learning for GME Stock Trading

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.20.0-orange.svg)](https://www.tensorflow.org/)
[![Stable-Baselines3](https://img.shields.io/badge/Stable--Baselines3-2.7.1-green.svg)](https://stable-baselines3.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Un projet complet d'apprentissage par renforcement profond (Deep RL) pour le trading automatisÃ© d'actions GameStop (GME). Ce projet utilise l'algorithme **A2C (Advantage Actor-Critic)** pour apprendre des stratÃ©gies de trading optimales sur des donnÃ©es historiques rÃ©elles.

![Trading Banner](https://img.shields.io/badge/Trading-Reinforcement%20Learning-success)

## ğŸ“‹ Table des MatiÃ¨res

- [AperÃ§u du Projet](#-aperÃ§u-du-projet)
- [CaractÃ©ristiques](#-caractÃ©ristiques)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [RÃ©sultats](#-rÃ©sultats)
- [Structure du Projet](#-structure-du-projet)
- [Technologies UtilisÃ©es](#-technologies-utilisÃ©es)
- [MÃ©thodologie](#-mÃ©thodologie)
- [Performances](#-performances)
- [Contribution](#-contribution)
- [Licence](#-licence)
- [Auteur](#-auteur)

## ğŸ¯ AperÃ§u du Projet

Ce projet dÃ©montre l'application de l'apprentissage par renforcement au trading algorithmique. Un agent intelligent est entraÃ®nÃ© pour apprendre Ã  acheter et vendre des actions GME en maximisant les profits tout en minimisant les risques.

### Objectifs
- ğŸ“ˆ DÃ©velopper un agent RL capable de prendre des dÃ©cisions de trading optimales
- ğŸ“Š Analyser les performances par rapport Ã  des stratÃ©gies de rÃ©fÃ©rence (baseline)
- ğŸ§ª Comparer avec des stratÃ©gies traditionnelles (Buy & Hold, Moving Average)
- ğŸ“‰ GÃ©rer le risque avec une analyse de drawdown dÃ©taillÃ©e

## âœ¨ CaractÃ©ristiques

- **Algorithme A2C** : ImplÃ©mentation de l'Advantage Actor-Critic pour des dÃ©cisions de trading robustes
- **Environnement PersonnalisÃ©** : Utilisation de `gym-anytrading` avec des donnÃ©es GME rÃ©elles
- **Analyse Statistique ComplÃ¨te** :
  - Statistiques descriptives (moyenne, mÃ©diane, Ã©cart-type, skewness, kurtosis)
  - Distribution des prix et rendements
  - Matrice de corrÃ©lation
  - Analyse de la volatilitÃ© mobile
  - Calcul du drawdown maximum
  - Tests de normalitÃ© (Shapiro-Wilk)
- **Visualisations AvancÃ©es** :
  - Graphiques OHLC interactifs
  - Heatmap de corrÃ©lation
  - Courbes de drawdown
  - Profil radar des performances
  - Q-Q Plots pour l'analyse de distribution
- **Benchmark Complet** : Comparaison avec stratÃ©gie alÃ©atoire et autres mÃ©thodes
- **Documentation en FranÃ§ais** : Explications dÃ©taillÃ©es ligne par ligne

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DONNÃ‰ES HISTORIQUES                      â”‚
â”‚              (GME Stock Data: Nov 2019 - Mar 2021)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PREPROCESSING & FEATURES                    â”‚
â”‚  â€¢ Conversion dates â€¢ Indexation â€¢ Normalisation            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ENVIRONNEMENT TRADING (GYM)                     â”‚
â”‚  â€¢ StocksEnv (window_size=5)                                â”‚
â”‚  â€¢ Frame_bound: (5,100) train / (90,110) test              â”‚
â”‚  â€¢ Actions: {Hold, Buy, Sell}                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT A2C                                 â”‚
â”‚  â€¢ Policy: MlpPolicy (Multi-Layer Perceptron)               â”‚
â”‚  â€¢ Learning Rate: 7e-4                                       â”‚
â”‚  â€¢ Gamma: 0.99                                               â”‚
â”‚  â€¢ N_steps: 5                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ENTRAÃNEMENT (1M timesteps)                     â”‚
â”‚  â€¢ ~10,526 Ã©pisodes                                         â”‚
â”‚  â€¢ DurÃ©e: 30-45 min sur CPU                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Ã‰VALUATION                              â”‚
â”‚  â€¢ Test sur pÃ©riode validation                              â”‚
â”‚  â€¢ Calcul mÃ©triques: Profit, Sharpe Ratio, Drawdown        â”‚
â”‚  â€¢ Comparaison avec baselines                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### PrÃ©requis
- Python 3.10 ou supÃ©rieur
- pip (gestionnaire de packages Python)
- Git

### Installation Rapide

```bash
# Cloner le repository
git clone https://github.com/votre-username/Reinforcement-Learning-for-Trading.git
cd Reinforcement-Learning-for-Trading

# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install pandas numpy matplotlib seaborn scipy
pip install stable-baselines3==2.7.1
pip install tensorflow==2.20.0
pip install gym==0.26.2
pip install gym-anytrading==2.0.0
```

### VÃ©rification de l'installation

```python
import gym
import gym_anytrading
from stable_baselines3 import A2C
import tensorflow as tf

print(f"Gym: {gym.__version__}")
print(f"TensorFlow: {tf.__version__}")
print("âœ… Installation rÃ©ussie!")
```

## ğŸ’» Utilisation

### 1. Lancer le Notebook

```bash
jupyter notebook "Reinforcement Learning GME Trading Tutorial.ipynb"
```

### 2. ExÃ©cution pas Ã  pas

Le notebook est organisÃ© en sections claires :

1. **Installation des packages** - VÃ©rification et installation des dÃ©pendances
2. **Chargement des donnÃ©es** - Import et prÃ©processing des donnÃ©es GME
3. **Analyse exploratoire** - Statistiques descriptives et visualisations
4. **Configuration de l'environnement** - CrÃ©ation de l'environnement de trading
5. **Test baseline** - Agent avec actions alÃ©atoires
6. **EntraÃ®nement A2C** - Apprentissage du modÃ¨le (1M timesteps)
7. **Ã‰valuation** - Test et comparaison des performances

### 3. Exemple de Code Rapide

```python
import pandas as pd
from gym_anytrading.envs import StocksEnv
from stable_baselines3 import A2C

# Charger les donnÃ©es
df = pd.read_csv('data/gmedata.csv')
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)

# CrÃ©er l'environnement
env = StocksEnv(df=df, frame_bound=(5, 100), window_size=5)

# EntraÃ®ner l'agent
model = A2C('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=1000000)

# Tester le modÃ¨le
env_test = StocksEnv(df=df, frame_bound=(90, 110), window_size=5)
obs, info = env_test.reset()
total_reward = 0

while True:
    action, _states = model.predict(obs)
    obs, reward, done, truncated, info = env_test.step(action)
    total_reward += reward
    if done or truncated:
        break

print(f"Profit Total: {total_reward:.2f}")
```

## ğŸ“Š RÃ©sultats

### Comparaison des StratÃ©gies

| StratÃ©gie | Profit Total (%) | Sharpe Ratio | Max Drawdown (%) | Win Rate (%) | Nb Trades |
|-----------|------------------|--------------|------------------|--------------|-----------|
| **Agent A2C** | **+18.73%** | **1.42** | **-17.9%** | **66.7%** | **23** |
| Agent AlÃ©atoire | -22.52% | -0.15 | -44.8% | 48.3% | 47 |
| Buy & Hold | +182.0% | 0.85 | -75.3% | - | 1 |
| Moving Average | +8.30% | 0.62 | -28.1% | 54.2% | 32 |

### Points ClÃ©s

âœ… **Performance SupÃ©rieure** : L'agent A2C surpasse la baseline alÃ©atoire de +41.25 points de profit

âœ… **Excellent Sharpe Ratio** : 1.42 indique un trÃ¨s bon ratio rendement/risque

âœ… **Gestion du Risque** : Drawdown maximum limitÃ© Ã  -17.9% (vs -44.8% pour baseline)

âœ… **Win Rate Ã‰levÃ©** : 66.7% des trades sont gagnants

âœ… **Trading Efficace** : Seulement 23 trades pour +18.73% de profit

### Visualisations

Le projet inclut des visualisations dÃ©taillÃ©es :
- ğŸ“ˆ Graphiques OHLC avec volume
- ğŸ“‰ Courbes de drawdown
- ğŸ¯ Profil radar des performances
- ğŸ“Š Distributions des rendements
- ğŸ”¥ Heatmap de corrÃ©lation
- ğŸ“‰ VolatilitÃ© mobile
- ğŸ“ Q-Q Plots pour tests de normalitÃ©

## ğŸ“ Structure du Projet

```
Reinforcement-Learning-for-Trading/
â”‚
â”œâ”€â”€ ğŸ““ Reinforcement Learning GME Trading Tutorial.ipynb
â”‚   â””â”€â”€ Notebook principal avec code et analyses complÃ¨tes
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â””â”€â”€ gmedata.csv
â”‚       â””â”€â”€ DonnÃ©es GME (Nov 2019 - Mar 2021, 350 jours)
â”‚
â”œâ”€â”€ ğŸ“„ README.md
â”‚   â””â”€â”€ Documentation complÃ¨te du projet
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt
â”‚   â””â”€â”€ Liste des dÃ©pendances Python
â”‚
â””â”€â”€ ğŸ“„ LICENSE
    â””â”€â”€ Licence MIT
```

## ğŸ› ï¸ Technologies UtilisÃ©es

### Frameworks & BibliothÃ¨ques

- **Python 3.10+** - Langage de programmation
- **TensorFlow 2.20.0** - Backend pour l'apprentissage profond
- **Stable-Baselines3 2.7.1** - ImplÃ©mentation des algorithmes RL
- **OpenAI Gym 0.26.2** - Environnement de simulation
- **gym-anytrading 2.0.0** - Environnement de trading spÃ©cialisÃ©
- **Pandas 2.3.3** - Manipulation de donnÃ©es
- **NumPy 2.3.5** - Calculs numÃ©riques
- **Matplotlib 3.10.8** - Visualisations
- **Seaborn 0.13.2** - Visualisations statistiques avancÃ©es
- **SciPy** - Tests statistiques

### Algorithme

**A2C (Advantage Actor-Critic)** :
- MÃ©thode policy gradient avec value function
- Architecture Actor-Critic pour stabilitÃ© accrue
- Learning rate adaptative
- ParallÃ©lisation des expÃ©riences

## ğŸ§ª MÃ©thodologie

### 1. Collecte des DonnÃ©es
- Source : MarketWatch
- PÃ©riode : 25 Nov 2019 - 31 Mar 2021
- FrÃ©quence : Quotidienne (350 jours de trading)
- Variables : Open, High, Low, Close, Volume

### 2. Preprocessing
- Conversion des dates en format datetime
- Indexation temporelle
- VÃ©rification de la qualitÃ© des donnÃ©es
- Calcul des rendements journaliers

### 3. Feature Engineering
- Window size de 5 jours (historique observÃ© par l'agent)
- Observation : Ã©tat du marchÃ© (prix, tendances)
- Actions possibles : {Hold, Buy, Sell}

### 4. Split Train/Test
- **Training** : Jours 5-100 (95 jours, 71.4%)
- **Validation** : Jours 90-110 (20 jours, 14.3%)
- **Test** : Jours 110-350 (240 jours, 68.6%)

### 5. EntraÃ®nement
- Total timesteps : 1,000,000
- Episodes : ~10,526
- DurÃ©e : 30-45 minutes sur CPU
- Politique : MlpPolicy (rÃ©seau de neurones)

### 6. Ã‰valuation
- MÃ©triques : Profit total, Sharpe Ratio, Max Drawdown, Win Rate
- Comparaison avec baseline alÃ©atoire
- Analyse de robustesse

## ğŸ“ˆ Performances

### MÃ©triques d'Ã‰valuation

**Profit Total** : Variation du capital de dÃ©but Ã  fin de pÃ©riode
```
Profit (%) = (Capital_final - Capital_initial) / Capital_initial Ã— 100
```

**Sharpe Ratio** : Mesure du rendement ajustÃ© au risque
```
Sharpe = (Rendement_moyen - Taux_sans_risque) / VolatilitÃ©
```
- < 1 : Mauvais
- 1-2 : Bon
- \> 2 : Excellent

**Max Drawdown** : Perte maximale depuis un pic
```
Drawdown = (Prix - Prix_pic) / Prix_pic Ã— 100
```

**Win Rate** : Pourcentage de trades gagnants
```
Win_Rate = Trades_gagnants / Total_trades Ã— 100
```

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voici comment vous pouvez contribuer :

1. **Fork** le projet
2. CrÃ©ez votre **branche de fonctionnalitÃ©** (`git checkout -b feature/AmazingFeature`)
3. **Committez** vos changements (`git commit -m 'Add some AmazingFeature'`)
4. **Pushez** vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une **Pull Request**

### IdÃ©es d'AmÃ©lioration
- [ ] Ajouter plus d'indicateurs techniques (RSI, MACD, Bollinger Bands)
- [ ] ImplÃ©menter d'autres algorithmes (PPO, DQN, TD3)
- [ ] Backtesting sur plusieurs actions
- [ ] Ajout de frais de transaction rÃ©alistes
- [ ] Optimisation des hyperparamÃ¨tres avec Optuna
- [ ] DÃ©ploiement avec FastAPI/Streamlit
- [ ] Trading en temps rÃ©el avec API broker

## ğŸ“œ Licence

Ce projet est sous licence **MIT**. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

**Votre Nom**
- GitHub: [@votre-username](https://github.com/votre-username)
- LinkedIn: [Votre Profil](https://linkedin.com/in/votre-profil)
- Email: votre.email@example.com

## ğŸ™ Remerciements

- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) pour l'implÃ©mentation des algorithmes RL
- [gym-anytrading](https://github.com/AminHP/gym-anytrading) pour l'environnement de trading
- [OpenAI Gym](https://github.com/openai/gym) pour le framework d'environnement
- MarketWatch pour les donnÃ©es GME

## ğŸ“š Ressources Additionnelles

- [Documentation Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [OpenAI Spinning Up in Deep RL](https://spinningup.openai.com/)
- [A2C Algorithm Paper](https://arxiv.org/abs/1602.01783)
- [Reinforcement Learning for Trading](https://www.google.com/search?q=reinforcement+learning+for+trading)

## âš ï¸ Disclaimer

**Ce projet est Ã  des fins Ã©ducatives uniquement.** Les performances passÃ©es ne garantissent pas les rÃ©sultats futurs. Ne considÃ©rez pas ce code comme un conseil financier. Faites toujours vos propres recherches avant d'investir de l'argent rÃ©el.

---

<div align="center">
â­ Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  lui donner une Ã©toile ! â­
</div>

---

**Made with â¤ï¸ and ğŸ¤– by [Votre Nom]**
